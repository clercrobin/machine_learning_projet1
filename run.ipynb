{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython import display\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from features import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate a model\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    \"\"\" Compute accuracy. \"\"\"\n",
    "    right = np.sum(y_pred == y)\n",
    "    wrong = len(y_pred) - right\n",
    "    accuracy = right / len(y)\n",
    "\n",
    "    print(\"Good prediction: %i/%i (%.3f%%)\\nWrong prediction: %i/%i (%.3f%%)\" %\n",
    "        (right, len(y), 100.0 * accuracy, wrong, len(y), 100.0 * (1-accuracy)))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the whole data\n",
    "\n",
    "# Pickle dataset for fast reload\n",
    "y_train, x_train, ids_train, headers = load_csv_data('data/train.csv')\n",
    "y_test, x_test, ids_test, headers_test = load_csv_data('data/test.csv')\n",
    "pickle.dump((y_train, x_train, ids_train, headers), open('train.pickle', 'wb'))\n",
    "pickle.dump((y_test, x_test, ids_test, headers_test), open('test.pickle', 'wb'))\n",
    "\n",
    "# Load dataset using pickle\n",
    "def reload_dataset():\n",
    "    global y_train, x_train, ids_train, headers, y_test, x_test, ids_test, headers_test\n",
    "    y_train, x_train, ids_train, headers = pickle.load(open('train.pickle', 'rb'))\n",
    "    y_test, x_test, ids_test, headers_test = pickle.load(open('test.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x10cfe5f28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAD8CAYAAAD9uIjPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d42d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation analysis\n",
    "\n",
    "correlation_matrix = np.corrcoef(x_train.T)\n",
    "\n",
    "plt.imshow(correlation_matrix)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processing(x, deg, jet_mod=False, jet_num=0, mean=None, std=None):\n",
    "    long_tails=[0, 1, 2, 3, 5, 8, 9, 10, 13, 16, 19, 21, 23, 26, 29]\n",
    "    # Get the valid and invalid values (-999)\n",
    "    missing_mask = x == -999\n",
    "    correct_mask = x != -999\n",
    "    \n",
    "    # Log transform all the long tails\n",
    "    for i in long_tails:\n",
    "        # Only the valid values\n",
    "        x[correct_mask[:,i],i] = np.log(1 + x[correct_mask[:,i],i])\n",
    "\n",
    "    # Difference between angles\n",
    "    angle = [15, 18, 20]\n",
    "    diff01 = np.abs(x[:,angle[0]] - x[:,angle[1]]).reshape((len(x), 1))\n",
    "    diff02 = np.abs(x[:,angle[0]] - x[:,angle[2]]).reshape((len(x), 1))\n",
    "    diff12 = np.abs(x[:,angle[1]] - x[:,angle[2]]).reshape((len(x), 1))\n",
    "    \n",
    "    x = np.hstack((x, diff01, diff02, diff12))\n",
    "            \n",
    "    # Exclude some invalid variables depending of the jet_num\n",
    "    if jet_mod:\n",
    "        features_excluded = [[4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28], [4, 5, 6, 12, 22, 26, 27, 28], [], []]\n",
    "        excepted = np.setdiff1d(np.arange(x.shape[1]), features_excluded[jet_num])\n",
    "        x = x[:,excepted]\n",
    "    #print(\"Remaining number of variables: \" + str(len(x[0])))\n",
    "    \n",
    "\n",
    "    # Standardize\n",
    "    x, mean, std = standardize(x, mean, std)\n",
    "\n",
    "    # Build polynomial features\n",
    "    x = build_poly(x, deg)\n",
    "    \n",
    "    return x, mean, std\n",
    "\n",
    "def show_x(x):\n",
    "    for i in range(len(x[0])):\n",
    "        array = x[:,i]\n",
    "        plt.hist(array, 250)\n",
    "        plt.title(\"Variable %i: %s\"%(i, headers[i+2]))\n",
    "        plt.show()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degré : 4\n",
      "Lambda : 1e-08\n",
      "Jet: 0\n",
      "Loss = 0.228392\n",
      "Good prediction: 16771/19983 (83.926%)\n",
      "Wrong prediction: 3212/19983 (16.074%)\n",
      "\n",
      "Global accuracy: 0.839263373868\n",
      "Lambda : 1.58489319246e-07\n",
      "Jet: 0\n",
      "Loss = 0.228561\n",
      "Good prediction: 16767/19983 (83.906%)\n",
      "Wrong prediction: 3216/19983 (16.094%)\n",
      "\n",
      "Global accuracy: 0.839063203723\n",
      "Lambda : 2.51188643151e-06\n",
      "Jet: 0\n",
      "Loss = 0.229342\n",
      "Good prediction: 16754/19983 (83.841%)\n",
      "Wrong prediction: 3229/19983 (16.159%)\n",
      "\n",
      "Global accuracy: 0.838412650753\n",
      "Lambda : 3.98107170553e-05\n",
      "Jet: 0\n",
      "Loss = 0.230906\n",
      "Good prediction: 16756/19983 (83.851%)\n",
      "Wrong prediction: 3227/19983 (16.149%)\n",
      "\n",
      "Global accuracy: 0.838512735825\n",
      "Lambda : 0.00063095734448\n",
      "Jet: 0\n",
      "Loss = 0.231872\n",
      "Good prediction: 16750/19983 (83.821%)\n",
      "Wrong prediction: 3233/19983 (16.179%)\n",
      "\n",
      "Global accuracy: 0.838212480609\n",
      "Lambda : 0.01\n",
      "Jet: 0\n",
      "Loss = 0.235415\n",
      "Good prediction: 16761/19983 (83.876%)\n",
      "Wrong prediction: 3222/19983 (16.124%)\n",
      "\n",
      "Global accuracy: 0.838762948506\n",
      "Degré : 5\n",
      "Lambda : 1e-08\n",
      "Jet: 0\n",
      "Loss = 0.226641\n",
      "Good prediction: 16799/19983 (84.066%)\n",
      "Wrong prediction: 3184/19983 (15.934%)\n",
      "\n",
      "Global accuracy: 0.84066456488\n",
      "Lambda : 1.58489319246e-07\n",
      "Jet: 0\n",
      "Loss = 0.226802\n",
      "Good prediction: 16797/19983 (84.056%)\n",
      "Wrong prediction: 3186/19983 (15.944%)\n",
      "\n",
      "Global accuracy: 0.840564479808\n",
      "Lambda : 2.51188643151e-06\n",
      "Jet: 0\n",
      "Loss = 0.227554\n",
      "Good prediction: 16796/19983 (84.051%)\n",
      "Wrong prediction: 3187/19983 (15.949%)\n",
      "\n",
      "Global accuracy: 0.840514437272\n",
      "Lambda : 3.98107170553e-05\n",
      "Jet: 0\n",
      "Loss = 0.228956\n",
      "Good prediction: 16773/19983 (83.936%)\n",
      "Wrong prediction: 3210/19983 (16.064%)\n",
      "\n",
      "Global accuracy: 0.83936345894\n",
      "Lambda : 0.00063095734448\n",
      "Jet: 0\n",
      "Loss = 0.229876\n",
      "Good prediction: 16762/19983 (83.881%)\n",
      "Wrong prediction: 3221/19983 (16.119%)\n",
      "\n",
      "Global accuracy: 0.838812991042\n",
      "Lambda : 0.01\n",
      "Jet: 0\n",
      "Loss = 0.233561\n",
      "Good prediction: 16774/19983 (83.941%)\n",
      "Wrong prediction: 3209/19983 (16.059%)\n",
      "\n",
      "Global accuracy: 0.839413501476\n",
      "Degré : 6\n",
      "Lambda : 1e-08\n",
      "Jet: 0\n",
      "Loss = 0.225461\n",
      "Good prediction: 16811/19983 (84.127%)\n",
      "Wrong prediction: 3172/19983 (15.873%)\n",
      "\n",
      "Global accuracy: 0.841265075314\n",
      "Lambda : 1.58489319246e-07\n",
      "Jet: 0\n",
      "Loss = 0.225632\n",
      "Good prediction: 16831/19983 (84.227%)\n",
      "Wrong prediction: 3152/19983 (15.773%)\n",
      "\n",
      "Global accuracy: 0.842265926037\n",
      "Lambda : 2.51188643151e-06\n",
      "Jet: 0\n",
      "Loss = 0.226304\n",
      "Good prediction: 16808/19983 (84.111%)\n",
      "Wrong prediction: 3175/19983 (15.889%)\n",
      "\n",
      "Global accuracy: 0.841114947706\n",
      "Lambda : 3.98107170553e-05\n",
      "Jet: 0\n",
      "Loss = 0.227368\n",
      "Good prediction: 16791/19983 (84.026%)\n",
      "Wrong prediction: 3192/19983 (15.974%)\n",
      "\n",
      "Global accuracy: 0.840264224591\n",
      "Lambda : 0.00063095734448\n",
      "Jet: 0\n",
      "Loss = 0.228124\n",
      "Good prediction: 16790/19983 (84.021%)\n",
      "Wrong prediction: 3193/19983 (15.979%)\n",
      "\n",
      "Global accuracy: 0.840214182055\n",
      "Lambda : 0.01\n",
      "Jet: 0\n",
      "Loss = 0.231722\n",
      "Good prediction: 16789/19983 (84.016%)\n",
      "Wrong prediction: 3194/19983 (15.984%)\n",
      "\n",
      "Global accuracy: 0.840164139519\n",
      "Degré : 7\n",
      "Lambda : 1e-08\n",
      "Jet: 0\n",
      "Loss = 0.224491\n",
      "Good prediction: 16830/19983 (84.222%)\n",
      "Wrong prediction: 3153/19983 (15.778%)\n",
      "\n",
      "Global accuracy: 0.842215883501\n",
      "Lambda : 1.58489319246e-07\n",
      "Jet: 0\n",
      "Loss = 0.224625\n",
      "Good prediction: 16834/19983 (84.242%)\n",
      "Wrong prediction: 3149/19983 (15.758%)\n",
      "\n",
      "Global accuracy: 0.842416053646\n",
      "Lambda : 2.51188643151e-06\n",
      "Jet: 0\n",
      "Loss = 0.225242\n",
      "Good prediction: 16834/19983 (84.242%)\n",
      "Wrong prediction: 3149/19983 (15.758%)\n",
      "\n",
      "Global accuracy: 0.842416053646\n",
      "Lambda : 3.98107170553e-05\n",
      "Jet: 0\n",
      "Loss = 0.226235\n",
      "Good prediction: 16829/19983 (84.217%)\n",
      "Wrong prediction: 3154/19983 (15.783%)\n",
      "\n",
      "Global accuracy: 0.842165840965\n",
      "Lambda : 0.00063095734448\n",
      "Jet: 0\n",
      "Loss = 0.226980\n",
      "Good prediction: 16812/19983 (84.132%)\n",
      "Wrong prediction: 3171/19983 (15.868%)\n",
      "\n",
      "Global accuracy: 0.84131511785\n",
      "Lambda : 0.01\n",
      "Jet: 0\n",
      "Loss = 0.230804\n",
      "Good prediction: 16814/19983 (84.142%)\n",
      "Wrong prediction: 3169/19983 (15.858%)\n",
      "\n",
      "Global accuracy: 0.841415202922\n",
      "Degré : 8\n",
      "Lambda : 1e-08\n",
      "Jet: 0\n",
      "Loss = 0.223701\n",
      "Good prediction: 16855/19983 (84.347%)\n",
      "Wrong prediction: 3128/19983 (15.653%)\n",
      "\n",
      "Global accuracy: 0.843466946905\n",
      "Lambda : 1.58489319246e-07\n",
      "Jet: 0\n",
      "Loss = 0.223822\n",
      "Good prediction: 16863/19983 (84.387%)\n",
      "Wrong prediction: 3120/19983 (15.613%)\n",
      "\n",
      "Global accuracy: 0.843867287194\n",
      "Lambda : 2.51188643151e-06\n",
      "Jet: 0\n",
      "Loss = 0.224416\n",
      "Good prediction: 16880/19983 (84.472%)\n",
      "Wrong prediction: 3103/19983 (15.528%)\n",
      "\n",
      "Global accuracy: 0.844718010309\n",
      "Lambda : 3.98107170553e-05\n",
      "Jet: 0\n",
      "Loss = 0.225384\n",
      "Good prediction: 16860/19983 (84.372%)\n",
      "Wrong prediction: 3123/19983 (15.628%)\n",
      "\n",
      "Global accuracy: 0.843717159586\n",
      "Lambda : 0.00063095734448\n",
      "Jet: 0\n",
      "Loss = 0.226106\n",
      "Good prediction: 16841/19983 (84.277%)\n",
      "Wrong prediction: 3142/19983 (15.723%)\n",
      "\n",
      "Global accuracy: 0.842766351399\n",
      "Lambda : 0.01\n",
      "Jet: 0\n",
      "Loss = 0.230135\n",
      "Good prediction: 16823/19983 (84.187%)\n",
      "Wrong prediction: 3160/19983 (15.813%)\n",
      "\n",
      "Global accuracy: 0.841865585748\n",
      "Degré : 9\n",
      "Lambda : 1e-08\n",
      "Jet: 0\n",
      "Loss = 0.352521\n",
      "Good prediction: 16866/19983 (84.402%)\n",
      "Wrong prediction: 3117/19983 (15.598%)\n",
      "\n",
      "Global accuracy: 0.844017414803\n",
      "Lambda : 1.58489319246e-07\n",
      "Jet: 0\n",
      "Loss = 0.222977\n",
      "Good prediction: 16882/19983 (84.482%)\n",
      "Wrong prediction: 3101/19983 (15.518%)\n",
      "\n",
      "Global accuracy: 0.844818095381\n",
      "Lambda : 2.51188643151e-06\n",
      "Jet: 0\n",
      "Loss = 0.223566\n",
      "Good prediction: 16891/19983 (84.527%)\n",
      "Wrong prediction: 3092/19983 (15.473%)\n",
      "\n",
      "Global accuracy: 0.845268478206\n",
      "Lambda : 3.98107170553e-05\n",
      "Jet: 0\n",
      "Loss = 0.224520\n",
      "Good prediction: 16858/19983 (84.362%)\n",
      "Wrong prediction: 3125/19983 (15.638%)\n",
      "\n",
      "Global accuracy: 0.843617074513\n",
      "Lambda : 0.00063095734448\n",
      "Jet: 0\n",
      "Loss = 0.225240\n",
      "Good prediction: 16853/19983 (84.337%)\n",
      "Wrong prediction: 3130/19983 (15.663%)\n",
      "\n",
      "Global accuracy: 0.843366861833\n",
      "Lambda : 0.01\n",
      "Jet: 0\n",
      "Loss = 0.229309\n",
      "Good prediction: 16829/19983 (84.217%)\n",
      "Wrong prediction: 3154/19983 (15.783%)\n",
      "\n",
      "Global accuracy: 0.842165840965\n",
      "Degré : 10\n",
      "Lambda : 1e-08\n",
      "Jet: 0\n",
      "Loss = 0.222965\n",
      "Good prediction: 16873/19983 (84.437%)\n",
      "Wrong prediction: 3110/19983 (15.563%)\n",
      "\n",
      "Global accuracy: 0.844367712556\n",
      "Lambda : 1.58489319246e-07\n",
      "Jet: 0\n",
      "Loss = 0.222422\n",
      "Good prediction: 16876/19983 (84.452%)\n",
      "Wrong prediction: 3107/19983 (15.548%)\n",
      "\n",
      "Global accuracy: 0.844517840164\n",
      "Lambda : 2.51188643151e-06\n",
      "Jet: 0\n",
      "Loss = 0.222999\n",
      "Good prediction: 16871/19983 (84.427%)\n",
      "Wrong prediction: 3112/19983 (15.573%)\n",
      "\n",
      "Global accuracy: 0.844267627483\n",
      "Lambda : 3.98107170553e-05\n",
      "Jet: 0\n",
      "Loss = 0.223955\n",
      "Good prediction: 16850/19983 (84.322%)\n",
      "Wrong prediction: 3133/19983 (15.678%)\n",
      "\n",
      "Global accuracy: 0.843216734224\n",
      "Lambda : 0.00063095734448\n",
      "Jet: 0\n",
      "Loss = 0.224744\n",
      "Good prediction: 16849/19983 (84.317%)\n",
      "Wrong prediction: 3134/19983 (15.683%)\n",
      "\n",
      "Global accuracy: 0.843166691688\n",
      "Lambda : 0.01\n",
      "Jet: 0\n",
      "Loss = 0.229009\n",
      "Good prediction: 16838/19983 (84.262%)\n",
      "Wrong prediction: 3145/19983 (15.738%)\n",
      "\n",
      "Global accuracy: 0.84261622379\n",
      "Degré : 11\n",
      "Lambda : 1e-08\n",
      "Jet: 0\n",
      "Loss = 0.221567\n",
      "Good prediction: 16897/19983 (84.557%)\n",
      "Wrong prediction: 3086/19983 (15.443%)\n",
      "\n",
      "Global accuracy: 0.845568733423\n",
      "Lambda : 1.58489319246e-07\n",
      "Jet: 0\n",
      "Loss = 0.221686\n",
      "Good prediction: 16900/19983 (84.572%)\n",
      "Wrong prediction: 3083/19983 (15.428%)\n",
      "\n",
      "Global accuracy: 0.845718861032\n",
      "Lambda : 2.51188643151e-06\n",
      "Jet: 0\n",
      "Loss = 0.222264\n",
      "Good prediction: 16897/19983 (84.557%)\n",
      "Wrong prediction: 3086/19983 (15.443%)\n",
      "\n",
      "Global accuracy: 0.845568733423\n",
      "Lambda : 3.98107170553e-05\n",
      "Jet: 0\n",
      "Loss = 0.223244\n",
      "Good prediction: 16884/19983 (84.492%)\n",
      "Wrong prediction: 3099/19983 (15.508%)\n",
      "\n",
      "Global accuracy: 0.844918180453\n",
      "Lambda : 0.00063095734448\n",
      "Jet: 0\n",
      "Loss = 0.224078\n",
      "Good prediction: 16870/19983 (84.422%)\n",
      "Wrong prediction: 3113/19983 (15.578%)\n",
      "\n",
      "Global accuracy: 0.844217584947\n",
      "Lambda : 0.01\n",
      "Jet: 0\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "reload_dataset()\n",
    "\n",
    "# Parameters\n",
    "ratio = 0.8\n",
    "deg = 1\n",
    "accuracies = []\n",
    "numbers = []\n",
    "\n",
    "# We train a classifier for each \"jet\"\n",
    "\n",
    "for deg in range(4,20):\n",
    "    print(\"Degré : \" + str(deg))\n",
    "    for lambda_ in np.logspace(-8,-2,6):\n",
    "        print(\"Lambda : \" + str(lambda_))\n",
    "        accuracies = []\n",
    "        numbers = []\n",
    "        for i in range(1):\n",
    "            print(\"Jet: \" + str(i))\n",
    "            # Select the corresponding rows\n",
    "            jet_mask_train = x_train[:,22] == i\n",
    "            jet_mask_test = x_test[:,22] == i\n",
    "            if i == 2 and False:\n",
    "                # 2 and 3 are treated the same way\n",
    "                jet_mask_train = np.asarray(x_train[:,22]==i) + np.asarray(x_train[:,22]==3) \n",
    "                jet_mask_test = np.asarray(x_test[:,22]==i) + np.asarray(x_test[:,22]==3) \n",
    "\n",
    "            x_jet_train, x_jet_test = x_train[jet_mask_train], x_test[jet_mask_test]\n",
    "            y_jet_train, y_jet_test = y_train[jet_mask_train], y_test[jet_mask_test]\n",
    "\n",
    "            # Process features\n",
    "            x_jet_train, mean, std = processing(x_jet_train, deg, True, i)\n",
    "            x_jet_test, _, _ = processing(x_jet_test, deg, True, i, mean, std)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Split data (create a validation set)\n",
    "            x_jet_train, y_jet_train, x_jet_valid, y_jet_valid = split_data(x_jet_train, y_jet_train, ratio)\n",
    "\n",
    "            # Training\n",
    "            #w,loss =least_squares(y_jet_train, x_jet_train)\n",
    "            #w, loss = least_squares_SGD(y_jet_train, x_jet_train, np.zeros(x_jet_train.shape[1]), 200, 3e-2)\n",
    "            w, loss = ridge_regression(y_jet_train,x_jet_train, lambda_)\n",
    "            #w, loss = reg_logistic_regression_SGD((y_train2 == 1).astype(float), train_processed_poly, 1e-5,\n",
    "            #\tnp.zeros(train_processed_poly.shape[1]), 2000, 1e-7)\n",
    "            print(\"Loss = %f\"%(loss))\n",
    "\n",
    "            # Prediction\n",
    "            y_jet_valid_pred = predict_labels(w, x_jet_valid)\n",
    "            #y_jet_test_pred = predict_labels(w, x_jet_test)\n",
    "\n",
    "            # Evaluation\n",
    "            accuracies.append(accuracy(y_jet_valid, y_jet_valid_pred))\n",
    "            numbers.append(len(y_jet_valid))\n",
    "            #create_csv_submission(ids_test[jet_mask_test], y_jet_test_pred, \"submission\" + str(i))\n",
    "\n",
    "\n",
    "        print('\\nGlobal accuracy:', np.dot(accuracies, numbers) / np.sum(numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
