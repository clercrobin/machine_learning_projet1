{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython import display\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from features import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate a model\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    \"\"\" Compute accuracy. \"\"\"\n",
    "    right = np.sum(y_pred == y)\n",
    "    wrong = len(y_pred) - right\n",
    "    accuracy = right / len(y)\n",
    "\n",
    "    #print(\"Good prediction: %i/%i (%.3f%%)\\nWrong prediction: %i/%i (%.3f%%)\" %\n",
    "        #(right, len(y), 100.0 * accuracy, wrong, len(y), 100.0 * (1-accuracy)))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the whole data\n",
    "\n",
    "# Pickle dataset for fast reload\n",
    "y_train, x_train, ids_train, headers = load_csv_data('data/train.csv')\n",
    "y_test, x_test, ids_test, headers_test = load_csv_data('data/test.csv')\n",
    "pickle.dump((y_train, x_train, ids_train, headers), open('train.pickle', 'wb'))\n",
    "pickle.dump((y_test, x_test, ids_test, headers_test), open('test.pickle', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset using pickle\n",
    "def reload_dataset():\n",
    "    global y_train, x_train, ids_train, headers, y_test, x_test, ids_test, headers_test\n",
    "    y_train, x_train, ids_train, headers = pickle.load(open('train.pickle', 'rb'))\n",
    "    y_test, x_test, ids_test, headers_test = pickle.load(open('test.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x10cfe5f28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAD8CAYAAAD9uIjPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d42d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation analysis\n",
    "\n",
    "correlation_matrix = np.corrcoef(x_train.T)\n",
    "\n",
    "plt.imshow(correlation_matrix)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed=1):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    totalLength = y.shape[0]\n",
    "    intervalLength = int(totalLength / k_fold) # Length of an internval\n",
    "    np.random.seed()\n",
    "    indices = np.random.permutation(totalLength)\n",
    "    k_indices = [indices[k * intervalLength: (k + 1) * intervalLength] for k in range(k_fold)]\n",
    "    #print(k_indices)\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processing(x, deg, jet_mod=False, jet_num=0, mean=None, std=None):\n",
    "    long_tails=[0, 1, 2, 3, 5, 8, 9, 10, 13, 16, 19, 21, 23, 26, 29]\n",
    "    # Get the valid and invalid values (-999)\n",
    "    missing_mask = x == -999\n",
    "    correct_mask = x != -999\n",
    "    \n",
    "    # Log transform all the long tails\n",
    "    for i in long_tails:\n",
    "        # Only the valid values\n",
    "        x[correct_mask[:,i],i] = np.log(1 + x[correct_mask[:,i],i])\n",
    "\n",
    "    # Difference between angles\n",
    "    angle = [15, 18, 20]\n",
    "    diff01 = np.abs(x[:,angle[0]] - x[:,angle[1]]).reshape((len(x), 1))\n",
    "    diff02 = np.abs(x[:,angle[0]] - x[:,angle[2]]).reshape((len(x), 1))\n",
    "    diff12 = np.abs(x[:,angle[1]] - x[:,angle[2]]).reshape((len(x), 1))\n",
    "    \n",
    "    x = np.hstack((x, diff01, diff02, diff12))\n",
    "            \n",
    "    # Exclude some invalid variables depending of the jet_num\n",
    "    if jet_mod:\n",
    "        features_excluded = [[4, 5, 6, 12, 15,18,20, 22, 23, 24, 25, 26, 27, 28], [4, 5, 6, 12, 15,18,20, 22,25, 26, 27, 28], [15,18,20,25,28], [15,18,20,25,28]]\n",
    "        excepted = np.setdiff1d(np.arange(x.shape[1]), features_excluded[jet_num])\n",
    "        x = x[:,excepted]\n",
    "    #print(\"Remaining number of variables: \" + str(len(x[0])))\n",
    "    \n",
    "\n",
    "    # Standardize\n",
    "    x, mean, std = standardize(x, mean, std)\n",
    "\n",
    "    # Build polynomial features\n",
    "    x = build_poly(x, deg)\n",
    "    \n",
    "    return x, mean, std\n",
    "\n",
    "def show_x(x):\n",
    "    for i in range(len(x[0])):\n",
    "        array = x[:,i]\n",
    "        plt.hist(array, 250)\n",
    "        plt.title(\"Variable %i: %s\"%(i, headers[i+2]))\n",
    "        plt.show()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet: 2\n",
      "Degré : 1\n",
      "Lambda : 1e-11\n",
      "New best accuracy !!!!!!!!!!!!!!! Score : 0.713744141163 Lambda : 1e-11 Gamma : 1e-08 Degre : 1 Jet : 2\n",
      "Tested but not opti : 0.713744141163\n",
      "New best accuracy !!!!!!!!!!!!!!! Score : 0.717231872071 Lambda : 1e-11 Gamma : 1e-07 Degre : 1 Jet : 2\n",
      "Tested but not opti : 0.717231872071\n",
      "New best accuracy !!!!!!!!!!!!!!! Score : 0.736931348222 Lambda : 1e-11 Gamma : 1e-06 Degre : 1 Jet : 2\n",
      "Tested but not opti : 0.736931348222\n",
      "New best accuracy !!!!!!!!!!!!!!! Score : 0.773090708575 Lambda : 1e-11 Gamma : 1e-05 Degre : 1 Jet : 2\n",
      "Tested but not opti : 0.773090708575\n",
      "New best accuracy !!!!!!!!!!!!!!! Score : 0.809360352909 Lambda : 1e-11 Gamma : 0.0001 Degre : 1 Jet : 2\n",
      "Tested but not opti : 0.809360352909\n",
      "New best accuracy !!!!!!!!!!!!!!! Score : 0.814157706093 Lambda : 1e-11 Gamma : 0.001 Degre : 1 Jet : 2\n",
      "Tested but not opti : 0.814157706093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinclerc/Dropbox/EPFL/Machine learning/ML_course/projects/project1/machine_learning_projet1/implementations.py:105: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(np.dot(y.T, np.log(p)) + np.dot((1-y).T, np.log(1-p))) + lambda_*np.dot(w.T, w)\n",
      "/Users/robinclerc/Dropbox/EPFL/Machine learning/ML_course/projects/project1/machine_learning_projet1/implementations.py:80: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested but not opti : 0.757609594706\n",
      "Tested but not opti : 0.751075268817\n",
      "Tested but not opti : 0.745864350703\n",
      "Lambda : 1.33352143216e-10\n",
      "Tested but not opti : 0.710697546181\n",
      "Tested but not opti : 0.714736696995\n",
      "Tested but not opti : 0.734201819686\n",
      "Tested but not opti : 0.768582850841\n",
      "Tested but not opti : 0.806575682382\n",
      "Tested but not opti : 0.810256410256\n",
      "Tested but not opti : 0.754025365316\n",
      "Tested but not opti : 0.744306589468\n",
      "Tested but not opti : 0.761276537083\n",
      "Lambda : 1.77827941004e-09\n",
      "Tested but not opti : 0.711276537083\n",
      "Tested but not opti : 0.714474772539\n",
      "Tested but not opti : 0.733677970775\n",
      "Tested but not opti : 0.767838433967\n",
      "Tested but not opti : 0.8052936311\n",
      "Tested but not opti : 0.810904328646\n",
      "Tested but not opti : 0.761304108078\n",
      "Tested but not opti : 0.750096498484\n",
      "Tested but not opti : 0.744251447477\n",
      "Lambda : 2.37137370566e-08\n",
      "Tested but not opti : 0.710959470637\n",
      "Tested but not opti : 0.714612627516\n",
      "Tested but not opti : 0.733388475324\n",
      "Tested but not opti : 0.766735594155\n",
      "Tested but not opti : 0.805031706645\n",
      "Tested but not opti : 0.812751585332\n",
      "Tested but not opti : 0.752302178109\n",
      "Tested but not opti : 0.756796250345\n",
      "Tested but not opti : 0.736738351254\n",
      "Lambda : 3.16227766017e-07\n",
      "Tested but not opti : 0.710683760684\n",
      "Tested but not opti : 0.714130135098\n",
      "Tested but not opti : 0.733278191343\n",
      "Tested but not opti : 0.766583953681\n",
      "Tested but not opti : 0.803956437827\n",
      "Tested but not opti : 0.810283981252\n",
      "Tested but not opti : 0.765453542873\n",
      "Tested but not opti : 0.742721257237\n",
      "Tested but not opti : 0.733636614282\n",
      "Lambda : 4.21696503429e-06\n",
      "Tested but not opti : 0.710835401158\n",
      "Tested but not opti : 0.71406120761\n",
      "Tested but not opti : 0.733126550868\n",
      "Tested but not opti : 0.766515026192\n",
      "Tested but not opti : 0.804039150813\n",
      "Tested but not opti : 0.812379376896\n",
      "Tested but not opti : 0.758836503998\n",
      "Tested but not opti : 0.740736145575\n",
      "Tested but not opti : 0.748221670802\n",
      "Lambda : 5.6234132519e-05\n",
      "Tested but not opti : 0.710849186656\n",
      "Tested but not opti : 0.713937138131\n",
      "Tested but not opti : 0.732961124897\n",
      "Tested but not opti : 0.766184174249\n",
      "Tested but not opti : 0.804025365316\n",
      "Tested but not opti : 0.81105596912\n",
      "Tested but not opti : 0.764695340502\n",
      "Tested but not opti : 0.746415770609\n",
      "Tested but not opti : 0.74758753791\n",
      "Lambda : 0.000749894209332\n",
      "Tested but not opti : 0.710890543149\n",
      "Tested but not opti : 0.713771712159\n",
      "Tested but not opti : 0.732988695892\n",
      "Tested but not opti : 0.766556382685\n",
      "Tested but not opti : 0.804149434795\n",
      "Tested but not opti : 0.810187482768\n",
      "Tested but not opti : 0.760380479735\n",
      "Tested but not opti : 0.753901295837\n",
      "Tested but not opti : 0.736255858837\n",
      "Lambda : 0.01\n",
      "Tested but not opti : 0.710421836228\n",
      "Tested but not opti : 0.713702784671\n",
      "Tested but not opti : 0.732905982906\n",
      "Tested but not opti : 0.766087675765\n",
      "Tested but not opti : 0.803859939344\n",
      "Tested but not opti : 0.808036945134\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "reload_dataset()\n",
    "\n",
    "# Parameters\n",
    "ratio = 0.8\n",
    "deg = 1\n",
    "k_fold = 10\n",
    "\n",
    "accuracies = []\n",
    "numbers = []\n",
    "\n",
    "\n",
    "mon_fichier = open(\"fichier.txt\", \"w\") # Argh j'ai tout écrasé !\n",
    "\n",
    "\n",
    "\n",
    "# We train a classifier for each \"jet\"\n",
    "for i in range(2,-1,-1):\n",
    "    print(\"Jet: \" + str(i))\n",
    "    best = 0\n",
    "    best_lambda = 0\n",
    "    best_degree = 0\n",
    "    best_gamma = 0\n",
    "    # Select the corresponding rows\n",
    "    jet_mask_train = x_train[:,22] == i\n",
    "    jet_mask_test = x_test[:,22] == i\n",
    "    if i == 2 :\n",
    "        # 2 and 3 are treated the same way\n",
    "        jet_mask_train = np.asarray(x_train[:,22]==i) + np.asarray(x_train[:,22]==3) \n",
    "        jet_mask_test = np.asarray(x_test[:,22]==i) + np.asarray(x_test[:,22]==3) \n",
    "        \n",
    "    x_jet_train, x_jet_test = x_train[jet_mask_train], x_test[jet_mask_test]\n",
    "    y_jet_train, y_jet_test = y_train[jet_mask_train], y_test[jet_mask_test]\n",
    "    \n",
    "    k_indices = build_k_indices(y_jet_train, k_fold)\n",
    "\n",
    "    for deg in range(1,20,1):\n",
    "        print(\"Degré : \" + str(deg))\n",
    "        \n",
    "        for lambda_ in np.logspace(-11,-2,9):\n",
    "            print(\"Lambda : \" + str(lambda_))\n",
    "            \n",
    "            for gamma in np.logspace(-8,0,9):\n",
    "            \n",
    "    #             if i ==0:\n",
    "    #                 deg=10\n",
    "    #                 lambda_=10**(-7)\n",
    "    #             elif i ==1:\n",
    "    #                 deg=10\n",
    "    #                 lambda_=10**(-10)\n",
    "    #             elif i ==2:\n",
    "    #                 deg=9\n",
    "    #                 lambda_=10**(-8)\n",
    "    #             elif i ==3:\n",
    "    #                 deg=12\n",
    "    #                 lambda_=4*10**(-7)\n",
    "\n",
    "                accuracies = []\n",
    "                numbers = []\n",
    "\n",
    "                # Process features\n",
    "                x_jet_train_processed, mean, std = processing(x_jet_train, deg, True, i)\n",
    "    #             x_jet_test_processed, _, _ = processing(x_jet_test, deg, True, i, mean, std)\n",
    "\n",
    "                # Split data (create a validation set)\n",
    "                #x_jet_train_current, y_jet_train_current, x_jet_valid_current, y_jet_valid_current = split_data(x_jet_train_current, y_jet_train, ratio)\n",
    "\n",
    "                for k in range(k_fold):\n",
    "                    new_accuracy = cross_validation(y_jet_train, x_jet_train_processed, k_indices, k, lambda_, gamma)\n",
    "                    accuracies.append(new_accuracy)\n",
    "\n",
    "                # Training\n",
    "                #w,loss =least_squares(y_jet_train, x_jet_train)\n",
    "                #w, loss = least_squares_SGD(y_jet_train, x_jet_train, np.zeros(x_jet_train.shape[1]), 200, 3e-2)\n",
    "                w, loss = ridge_regression(y_jet_train,x_jet_train_processed, lambda_)\n",
    "                #w, loss = reg_logistic_regression_SGD((y_train2 == 1).astype(float), train_processed_poly, 1e-5,\n",
    "                #\tnp.zeros(train_processed_poly.shape[1]), 2000, 1e-7)\n",
    "                #print(\"Loss = %f\"%(loss))\n",
    "\n",
    "                # Prediction\n",
    "                #y_jet_valid_pred = predict_labels(w, x_jet_valid)\n",
    "    #             y_jet_test_pred = predict_labels(w, x_jet_test_processed)\n",
    "\n",
    "                # Evaluation\n",
    "                new_accuracy = np.asarray(accuracies).mean()\n",
    "                if new_accuracy>best:\n",
    "                    best=new_accuracy\n",
    "                    print(\"New best accuracy !!!!!!!!!!!!!!! Score : \" + str(new_accuracy) +\" Lambda : \" +str(lambda_)+\" Gamma : \" +str(gamma) + \" Degre : \"+str(deg) + \" Jet : \" + str(i) )\n",
    "                    mon_fichier.write(\"New best accuracy !!!!!!!!!!!!!!! Score : \" + str(new_accuracy) +\" Lambda : \" +str(lambda_)+\" Gamma : \" +str(gamma) + \" Degre : \"+str(deg) + \" Jet : \" + str(i)  )\n",
    "                    best_lambda = lambda_\n",
    "                    best_degree = deg\n",
    "                accuracies.append(new_accuracy)\n",
    "                print(\"Tested but not opti : \" + str(new_accuracy))\n",
    "    #             #numbers.append(len(y_jet_valid))\n",
    "    #             create_csv_submission(ids_test[jet_mask_test], y_jet_test_pred, \"submission\" + str(i))\n",
    "    #             break\n",
    "    #         break\n",
    "    #    break\n",
    "\n",
    "mon_fichier.close()\n",
    "        #print('\\nGlobal accuracy:', np.dot(accuracies, numbers) / np.sum(numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3,-1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 9]\n"
     ]
    }
   ],
   "source": [
    "print(np.square(np.asarray([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id' 'Prediction' 'DER_mass_MMC' 'DER_mass_transverse_met_lep'\n",
      " 'DER_mass_vis' 'DER_pt_h' 'DER_deltaeta_jet_jet' 'DER_mass_jet_jet'\n",
      " 'DER_prodeta_jet_jet' 'DER_deltar_tau_lep' 'DER_pt_tot' 'DER_sum_pt'\n",
      " 'DER_pt_ratio_lep_tau' 'DER_met_phi_centrality' 'DER_lep_eta_centrality'\n",
      " 'PRI_tau_pt' 'PRI_tau_eta' 'PRI_tau_phi' 'PRI_lep_pt' 'PRI_lep_eta'\n",
      " 'PRI_lep_phi' 'PRI_met' 'PRI_met_phi' 'PRI_met_sumet' 'PRI_jet_num'\n",
      " 'PRI_jet_leading_pt' 'PRI_jet_leading_eta' 'PRI_jet_leading_phi'\n",
      " 'PRI_jet_subleading_pt' 'PRI_jet_subleading_eta' 'PRI_jet_subleading_phi'\n",
      " 'PRI_jet_all_pt']\n"
     ]
    }
   ],
   "source": [
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
